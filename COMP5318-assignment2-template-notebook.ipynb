{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc4122b0",
   "metadata": {},
   "source": [
    "# COMP4318/5318 Assignment 2: Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9f508e",
   "metadata": {},
   "source": [
    "### Group number: 91  , SID1: 500209446 , SID2: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24652d1",
   "metadata": {},
   "source": [
    "This template notebook includes code to load the  dataset and a skeleton for the main sections that should be included in the notebook. Please stick to this struture for your submitted notebook.\n",
    "\n",
    "Please focus on making your code clear, with appropriate variable names and whitespace. Include comments and markdown text to aid the readability of your code where relevant. See the specification and marking criteria in the associated specification to guide you when completing your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1144e1",
   "metadata": {},
   "source": [
    "## Setup and dependencies\n",
    "Please use this section to list and set up all your required libraries/dependencies and your plotting environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11ab0507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import time\n",
    "# Make the notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "# Set up inline plotting and figure/axis labels\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras_tuner\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc35bce",
   "metadata": {},
   "source": [
    "## 1. Data loading, exploration, and preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7495a12e-9b9d-48f8-9d2a-abbd01c5a594",
   "metadata": {},
   "source": [
    "Code to load the dataset is provided in the following cell. Please proceed with your data exploration and preprocessing in the remainder of this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a991e631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training X: 18928 Y: 18928\n",
      "Number of testing X: 4732 Y: 4732\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset training and test sets as numpy arrays\n",
    "# assuming Assignment2Data folder is present in the same directory \n",
    "# as the notebook\n",
    "X_train = np.load('Assignment2Data/X_train.npy')\n",
    "y_train = np.load('Assignment2Data/y_train.npy')\n",
    "X_test = np.load('Assignment2Data/X_test.npy')\n",
    "y_test = np.load('Assignment2Data/y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8de98a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10] [1741  936  916  978 1562 1651 4201 1519 1546 1682 2196]\n"
     ]
    }
   ],
   "source": [
    "classes = np.unique(y_train)\n",
    "print(\"Class Labels: \", classes)\n",
    "print(f\"X_train shape : {X_train.shape}\\ny_train shape : {y_train.shape}\\nX_test shape : {X_test.shape}\\ny_test shape : {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c97ca0-15b8-4101-b6b5-b76431c3ca17",
   "metadata": {},
   "source": [
    "### Examples of preprocessed data\n",
    "Please print/display some examples of your preprocessed data here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8317ae97",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e694f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {}\n",
    "actual_labels = [\"bladder\",\"femur-left\",\"femur-right\",\"heart\",\"kidney-left\",\"kidney-right\",\"liver\",\"lung-left\",\"lung-right\",\"pancreas\",\"spleen\"]\n",
    "for i, num_label in enumerate(classes):\n",
    "    class_dict[num_label]= actual_labels[i]\n",
    "print(\"label dictionary :\",class_dict)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2,11, figsize=(17, 3))\n",
    "\n",
    "for i, ax in enumerate(axes[0,:]):\n",
    "    idx_tuple = np.where(y_train == classes[i])\n",
    "    idx = idx_tuple[0][0]\n",
    "    ax.imshow(X_train[idx], cmap='gray')\n",
    "    ax.set_title(f\"{classes[i]}: {actual_labels[classes[i]]}\")\n",
    "    ax.axis('off')\n",
    "for i, ax in enumerate(axes[1,:]):\n",
    "    idx_tuple = np.where(y_train == classes[i])\n",
    "    idx = idx_tuple[0][1]\n",
    "    ax.imshow(X_train[idx], cmap='gray')\n",
    "    ax.set_title(f\"{classes[i]}: {actual_labels[classes[i]]}\")\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "class_distribution = dict(zip(unique, counts))\n",
    "print(\"Class distribution in training data:\", class_distribution)\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.bar(class_distribution.keys(), class_distribution.values(), color='lightseagreen')\n",
    "plt.xlabel('Class Label')\n",
    "plt.ylabel('Number of Examples')\n",
    "plt.title('Class Distribution in Training Data')\n",
    "plt.xticks(list(class_distribution.keys()))  # ensure all class labels are shown\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cdfb19",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7963743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocessing(X_train, X_test):\n",
    "    X_train = X_train / 255\n",
    "    X_test = X_test / 255\n",
    "    X_train = X_train.reshape(-1, 28*28) # number of rows required to keep the array's total size constant based on the other dimensions\n",
    "    X_test = X_test.reshape(-1, 28*28)\n",
    "    return X_train, X_test\n",
    "\n",
    "def apply_pca(X_train, X_test, n_components=0.95):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    print(X_train_pca.shape)\n",
    "    print(X_test_pca.shape)\n",
    "    return X_train_pca, X_test_pca\n",
    "    \n",
    "def expand_dimension(x_train,x_val,x_test):\n",
    "    X_train_split = np.expand_dims(X_train_split, -1)\n",
    "    X_val = np.expand_dims(X_val, -1)\n",
    "    X_test = np.expand_dims(X_test, -1)\n",
    "    return X_train_split,X_val,X_test\n",
    "\n",
    "def plot_explained_variance(X_train_normalized):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    pca_model = PCA()\n",
    "    X_train_pca = pca_model.fit_transform(X_train_normalized)\n",
    "    explained_variance = np.cumsum(pca_model.explained_variance_ratio_)\n",
    "\n",
    "    # Find the number of components to reach 95% variance\n",
    "    num_components = np.argmax(explained_variance >= 0.95) + 1  # plus one because np.argmax returns zero-based index\n",
    "\n",
    "    # Plot the explained variance and mark the point where it reaches 95%\n",
    "    plt.plot(explained_variance, label='Cumulative Explained Variance')\n",
    "    plt.scatter(num_components - 1, explained_variance[num_components - 1], color='red')  # Mark the 95% threshold point\n",
    "    plt.axhline(y=0.95, color='r', linestyle='--', label='95% Explained Variance')\n",
    "    plt.axvline(x=num_components - 1, color='r', linestyle='--')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('Explained Variance vs. Number of Components')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cd9737",
   "metadata": {},
   "source": [
    "###  For the following cell :\n",
    "###  1. PCA is not applied for MLP&CNN\n",
    "###  2. train_test_split is used only for MLP&CNN ( SVM uses grid-search )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eaa76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.  Normalization to range [0,1]\n",
    "X_train_normalized, X_test_normalized = image_preprocessing(X_train, X_test)\n",
    "\n",
    "# 2.  Apply Dimensionality reduction using PCA\n",
    "X_train_pca, X_test_pca = apply_pca(X_train_normalized, X_test_normalized)\n",
    "\n",
    "# 3. train and test spilt\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(X_train_normalized, y_train, stratify=y_train, train_size=0.8)\n",
    "\n",
    "# 4. Input data for neural network  å°é¸¡è®­ç»ƒç¥žç»ç½‘ç»œçš„å˜é‡ðŸ¤\n",
    "X_train_split, X_val, X_test = expand_dimension(X_train_split, X_val, X_test_normalized)\n",
    "\n",
    "# plot cumulative variance\n",
    "plot_explained_variance(X_train_normalized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81bb2b0",
   "metadata": {},
   "source": [
    "## 2. Algorithm design and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c873320",
   "metadata": {},
   "source": [
    "### 2.1 Algorithm of choice from first six weeks of course - Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c91bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = svm.SVC(kernel='rbf', C=10, gamma=0.001)\n",
    "svm_model.fit(X_train_pca, y_train)\n",
    "y_pred = svm_model.predict(X_test_pca)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of the SVM on test data:\", accuracy)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b895ee03",
   "metadata": {},
   "source": [
    "### 2.2 Fully connected neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8e24ee",
   "metadata": {},
   "source": [
    "#### 2.2.1 Define MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e54bdfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp_model(hyperparameters):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(28,28,1)))\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    hp_layers = hyperparameters.Int('layers', 1, 5)\n",
    "    for _ in hp_layers:\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                # Define the hyperparameter.\n",
    "                units=hyperparameters.Int(\"units\", min_value=32, max_value=512, step=32),\n",
    "                activation=hyperparameters.Choice(\"activation\", [\"relu\", \"tanh\"]),\n",
    "            )\n",
    "        )\n",
    "    model.add(layers.Dense(11, activation=\"softmax\"))\n",
    "    \n",
    "    learning_rate = hyperparameters.Float(\"lr\", min_value=1e-5, max_value=1e-3, sampling=\"log\")\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60351160",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = keras.models.Sequential([\n",
    "    keras.layers.Input(shape=(28,28,1)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(300, activation=\"tanh\"),\n",
    "    keras.layers.Dense(100, activation=\"tanh\"),\n",
    "    keras.layers.Dense(11, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "opt = keras.optimizers.SGD(learning_rate=1e-4)\n",
    "mlp_model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65837ec3",
   "metadata": {},
   "source": [
    "#### 2.2.2 Train MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2eb26466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.2797 - accuracy: 0.2221 - val_loss: 2.1263 - val_accuracy: 0.2644\n",
      "Epoch 2/100\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 2.0420 - accuracy: 0.3001 - val_loss: 1.9883 - val_accuracy: 0.3228\n",
      "Epoch 3/100\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.9179 - accuracy: 0.3618 - val_loss: 1.8807 - val_accuracy: 0.3735\n",
      "Epoch 4/100\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.8228 - accuracy: 0.4014 - val_loss: 1.8010 - val_accuracy: 0.4068\n",
      "Epoch 5/100\n",
      "474/474 [==============================] - 1s 2ms/step - loss: 1.7538 - accuracy: 0.4292 - val_loss: 1.7456 - val_accuracy: 0.4245\n",
      "Epoch 6/100\n",
      "380/474 [=======================>......] - ETA: 0s - loss: 1.7047 - accuracy: 0.4504"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmlp_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.10/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = mlp_model.fit(X_train_split, y_train_split, epochs=100,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d85d0a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63aa570f",
   "metadata": {},
   "source": [
    "### Convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ddc0dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = keras.Sequential([\n",
    "    \n",
    "    # Specify the input shape\n",
    "    keras.Input(shape=(28, 28, 1)),\n",
    "    \n",
    "    # Conv and pool block 1\n",
    "    keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    # Conv and pool block 2\n",
    "    keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    # Flatten and classify using dense output layer\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(11, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "cnn_model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])\n",
    "batch_size = 128\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0622057a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "119/119 [==============================] - 5s 40ms/step - loss: 3.0690 - accuracy: 0.4703 - val_loss: 1.0179 - val_accuracy: 0.6680\n",
      "Epoch 2/10\n",
      "119/119 [==============================] - 5s 39ms/step - loss: 0.9234 - accuracy: 0.6950 - val_loss: 0.6774 - val_accuracy: 0.7800\n",
      "Epoch 3/10\n",
      "119/119 [==============================] - 5s 40ms/step - loss: 0.6656 - accuracy: 0.7802 - val_loss: 0.5916 - val_accuracy: 0.8090\n",
      "Epoch 4/10\n",
      "119/119 [==============================] - 5s 40ms/step - loss: 0.5460 - accuracy: 0.8202 - val_loss: 0.5104 - val_accuracy: 0.8296\n",
      "Epoch 5/10\n",
      "119/119 [==============================] - 5s 39ms/step - loss: 0.4889 - accuracy: 0.8366 - val_loss: 0.4369 - val_accuracy: 0.8627\n",
      "Epoch 6/10\n",
      "119/119 [==============================] - 5s 39ms/step - loss: 0.4244 - accuracy: 0.8585 - val_loss: 0.4272 - val_accuracy: 0.8679\n",
      "Epoch 7/10\n",
      "119/119 [==============================] - 5s 40ms/step - loss: 0.3912 - accuracy: 0.8700 - val_loss: 0.4009 - val_accuracy: 0.8700\n",
      "Epoch 8/10\n",
      "119/119 [==============================] - 5s 40ms/step - loss: 0.3786 - accuracy: 0.8731 - val_loss: 0.3880 - val_accuracy: 0.8814\n",
      "Epoch 9/10\n",
      "119/119 [==============================] - 5s 40ms/step - loss: 0.3412 - accuracy: 0.8857 - val_loss: 0.3991 - val_accuracy: 0.8735\n",
      "Epoch 10/10\n",
      "119/119 [==============================] - 5s 40ms/step - loss: 0.3369 - accuracy: 0.8837 - val_loss: 0.3784 - val_accuracy: 0.8819\n"
     ]
    }
   ],
   "source": [
    "history = cnn_model.fit(X_train_split, y_train_split, batch_size=batch_size,\n",
    "                    epochs=epochs, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4911b1",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71085600",
   "metadata": {},
   "source": [
    "### Algorithm of choice from first six weeks of course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a95c6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use subset of data to do hyperparameter tuning\n",
    "print(X_train_pca.shape)\n",
    "X_train_pca = X_train_pca[:5000,:]\n",
    "y_train = y_train[:5000]\n",
    "\n",
    "param_grid = [\n",
    "    {'kernel': ['poly'], 'C': [0.1, 1, 10], 'gamma': [0.001, 0.01], 'degree': [2, 3, 4]},\n",
    "    {'kernel': ['rbf'], 'C': [0.1, 1, 10], 'gamma': [0.001, 0.01]}\n",
    "]\n",
    "\n",
    "svm_tuning = svm.SVC(class_weight='balanced')\n",
    "\n",
    "grid_search = GridSearchCV(svm_tuning, param_grid, cv=5, scoring='accuracy', verbose=2)\n",
    "\n",
    "grid_search.fit(X_train_pca, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872f07c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_search.predict(X_test_pca)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of the best SVM on test data:\", accuracy)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d2d615",
   "metadata": {},
   "source": [
    "### Fully connected neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab67ce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp_model(hyperparameters):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(28,28,1)))\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    hp_layers = hyperparameters.Int('num_layers', 1, 5)\n",
    "    for i in range(hp_layers):\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                # Define the hyperparameter.\n",
    "                units=hyperparameters.Int(f\"units_{i}\", min_value=32, max_value=512, step=32),\n",
    "                activation=hyperparameters.Choice(\"activation\", [\"relu\", \"tanh\"]),\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    if hyperparameters.Boolean(\"dropout\"):\n",
    "        model.add(layers.Dropout(rate=0.25))\n",
    "    model.add(layers.Dense(11, activation=\"softmax\"))\n",
    "\n",
    "    learning_rate = hyperparameters.Float(\"lr\", min_value=1e-5, max_value=1e-3, sampling=\"log\")\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4943b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 5\n",
      "num_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 5, 'step': 1, 'sampling': 'linear'}\n",
      "units_0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "activation (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh'], 'ordered': False}\n",
      "dropout (Boolean)\n",
      "{'default': False, 'conditions': []}\n",
      "lr (Float)\n",
      "{'default': 1e-05, 'conditions': [], 'min_value': 1e-05, 'max_value': 0.001, 'step': None, 'sampling': 'log'}\n"
     ]
    }
   ],
   "source": [
    "build_mlp_model(keras_tuner.HyperParameters())\n",
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    hypermodel=build_mlp_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=100,\n",
    "    overwrite=True,\n",
    "    directory=\"mlp_tuning\",\n",
    "    project_name=\"mlp_tuning\"\n",
    ")\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ea710b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 100 Complete [00h 00m 30s]\n",
      "val_accuracy: 0.8285789489746094\n",
      "\n",
      "Best val_accuracy So Far: 0.8520866632461548\n",
      "Total elapsed time: 01h 04m 38s\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train_split, y_train_split, epochs=20, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35c44773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 224)               175840    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 480)               108000    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 224)               107744    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 416)               93600     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 416)               173472    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 11)                4587      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 663,243\n",
      "Trainable params: 663,243\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "models = tuner.get_best_models(num_models=2)\n",
    "best_model = models[0]\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03b43a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(1)\n",
    "print(best_hps[0])\n",
    "model = build_mlp_model(best_hps[0])\n",
    "\n",
    "usualCallback = EarlyStopping()\n",
    "overfitCallback = EarlyStopping(monitor='loss', min_delta=0, patience = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1cb2787c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 2.0621 - accuracy: 0.5775\n",
      "Epoch 2/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.9185 - accuracy: 0.6989\n",
      "Epoch 3/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.7667 - accuracy: 0.7460\n",
      "Epoch 4/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.6723 - accuracy: 0.7762\n",
      "Epoch 5/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.6087 - accuracy: 0.7964\n",
      "Epoch 6/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.5648 - accuracy: 0.8107\n",
      "Epoch 7/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.5294 - accuracy: 0.8200\n",
      "Epoch 8/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.4701 - accuracy: 0.8391\n",
      "Epoch 9/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.4669 - accuracy: 0.8409\n",
      "Epoch 10/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.4145 - accuracy: 0.8593\n",
      "Epoch 11/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.3871 - accuracy: 0.8655\n",
      "Epoch 12/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.3563 - accuracy: 0.8794\n",
      "Epoch 13/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.3528 - accuracy: 0.8788\n",
      "Epoch 14/100\n",
      "592/592 [==============================] - 3s 4ms/step - loss: 0.3096 - accuracy: 0.8943\n",
      "Epoch 15/100\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 0.3000 - accuracy: 0.8984\n",
      "Epoch 16/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.2679 - accuracy: 0.9077\n",
      "Epoch 17/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.2554 - accuracy: 0.9128\n",
      "Epoch 18/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.2451 - accuracy: 0.9181\n",
      "Epoch 19/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.2175 - accuracy: 0.9247\n",
      "Epoch 20/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.2471 - accuracy: 0.9194\n",
      "Epoch 21/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.1900 - accuracy: 0.9340\n",
      "Epoch 22/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.2003 - accuracy: 0.9331\n",
      "Epoch 23/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.1910 - accuracy: 0.9358\n",
      "Epoch 24/100\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 0.1795 - accuracy: 0.9386\n",
      "Epoch 25/100\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 0.1773 - accuracy: 0.9409\n",
      "Epoch 26/100\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 0.1889 - accuracy: 0.9415\n",
      "Epoch 27/100\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 0.1325 - accuracy: 0.9560\n",
      "Epoch 28/100\n",
      "592/592 [==============================] - 3s 5ms/step - loss: 0.1480 - accuracy: 0.9498\n",
      "Epoch 29/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.1512 - accuracy: 0.9504\n",
      "Epoch 30/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.1358 - accuracy: 0.9564\n",
      "Epoch 31/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.1465 - accuracy: 0.9505\n",
      "Epoch 32/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.1320 - accuracy: 0.9567\n",
      "Epoch 33/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.1339 - accuracy: 0.9561\n",
      "Epoch 34/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.1083 - accuracy: 0.9654\n",
      "Epoch 35/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.1197 - accuracy: 0.9612\n",
      "Epoch 36/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.1128 - accuracy: 0.9622\n",
      "Epoch 37/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.1134 - accuracy: 0.9628\n",
      "Epoch 38/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.1211 - accuracy: 0.9610\n",
      "Epoch 39/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.1205 - accuracy: 0.9627\n",
      "Epoch 40/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0990 - accuracy: 0.9675\n",
      "Epoch 41/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0866 - accuracy: 0.9706\n",
      "Epoch 42/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.1304 - accuracy: 0.9577\n",
      "Epoch 43/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0670 - accuracy: 0.9787\n",
      "Epoch 44/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.1067 - accuracy: 0.9677\n",
      "Epoch 45/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0917 - accuracy: 0.9698\n",
      "Epoch 46/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0908 - accuracy: 0.9727\n",
      "Epoch 47/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.1018 - accuracy: 0.9697\n",
      "Epoch 48/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0775 - accuracy: 0.9743\n",
      "Epoch 49/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.1066 - accuracy: 0.9677\n",
      "Epoch 50/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0639 - accuracy: 0.9786\n",
      "Epoch 51/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.1168 - accuracy: 0.9629\n",
      "Epoch 52/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0838 - accuracy: 0.9739\n",
      "Epoch 53/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0496 - accuracy: 0.9819\n",
      "Epoch 54/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.1273 - accuracy: 0.9635\n",
      "Epoch 55/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0795 - accuracy: 0.9768\n",
      "Epoch 56/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0635 - accuracy: 0.9796\n",
      "Epoch 57/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0938 - accuracy: 0.9729\n",
      "Epoch 58/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0705 - accuracy: 0.9780\n",
      "Epoch 59/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0581 - accuracy: 0.9814\n",
      "Epoch 60/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0955 - accuracy: 0.9706\n",
      "Epoch 61/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0534 - accuracy: 0.9830\n",
      "Epoch 62/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.1306 - accuracy: 0.9675\n",
      "Epoch 63/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0535 - accuracy: 0.9818\n",
      "Epoch 64/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0819 - accuracy: 0.9750\n",
      "Epoch 65/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0505 - accuracy: 0.9833\n",
      "Epoch 66/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.1078 - accuracy: 0.9715\n",
      "Epoch 67/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0810 - accuracy: 0.9767\n",
      "Epoch 68/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0676 - accuracy: 0.9809\n",
      "Epoch 69/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0684 - accuracy: 0.9793\n",
      "Epoch 70/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0551 - accuracy: 0.9824\n",
      "Epoch 71/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0772 - accuracy: 0.9760\n",
      "Epoch 72/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0701 - accuracy: 0.9783\n",
      "Epoch 73/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0568 - accuracy: 0.9824\n",
      "Epoch 74/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0555 - accuracy: 0.9837\n",
      "Epoch 75/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0794 - accuracy: 0.9774\n",
      "Epoch 76/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0618 - accuracy: 0.9803\n",
      "Epoch 77/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0764 - accuracy: 0.9794\n",
      "Epoch 78/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0336 - accuracy: 0.9895\n",
      "Epoch 79/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0928 - accuracy: 0.9747\n",
      "Epoch 80/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0587 - accuracy: 0.9808\n",
      "Epoch 81/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0595 - accuracy: 0.9811\n",
      "Epoch 82/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0431 - accuracy: 0.9874\n",
      "Epoch 83/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0771 - accuracy: 0.9771\n",
      "Epoch 84/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0653 - accuracy: 0.9815\n",
      "Epoch 85/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0647 - accuracy: 0.9810\n",
      "Epoch 86/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0703 - accuracy: 0.9787\n",
      "Epoch 87/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0501 - accuracy: 0.9856\n",
      "Epoch 88/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0719 - accuracy: 0.9797\n",
      "Epoch 89/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0442 - accuracy: 0.9865\n",
      "Epoch 90/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0564 - accuracy: 0.9835\n",
      "Epoch 91/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0626 - accuracy: 0.9817\n",
      "Epoch 92/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0692 - accuracy: 0.9788\n",
      "Epoch 93/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0536 - accuracy: 0.9854\n",
      "Epoch 94/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0457 - accuracy: 0.9861\n",
      "Epoch 95/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0603 - accuracy: 0.9827\n",
      "Epoch 96/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0385 - accuracy: 0.9882\n",
      "Epoch 97/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0685 - accuracy: 0.9799\n",
      "Epoch 98/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0323 - accuracy: 0.9902\n",
      "Epoch 99/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0923 - accuracy: 0.9736\n",
      "Epoch 100/100\n",
      "592/592 [==============================] - 2s 4ms/step - loss: 0.0361 - accuracy: 0.9889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb1a6b9ac50>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, y=y_train, epochs=100, callbacks=[overfitCallback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7221ade3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57c76117",
   "metadata": {},
   "source": [
    "### Convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eead5c82",
   "metadata": {},
   "source": [
    "## 4. Final models\n",
    "In this section, please ensure to include cells to train each model with its best hyperparmater combination independently of the hyperparameter tuning cells, i.e. don't rely on the hyperparameter tuning cells having been run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e56a6c",
   "metadata": {},
   "source": [
    "### Algorithm of choice from first six weeks of course\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(best_params)\n",
    "\n",
    "\n",
    "best_svm_model = svm.SVC(C=best_params['C'],gamma=best_params['gamma'],kernel=best_params['kernel'],class_weight='balanced')\n",
    "best_svm_model.fit(X_train_pca, y_train)\n",
    "y_pred = best_svm_model.predict(X_test_pca)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of the best SVM on test data:\", accuracy)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc077832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns_heatmap = sns.heatmap(cm, annot=True, fmt=\".2f\" if normalize else \"d\", cmap=cmap,\n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plot_confusion_matrix(cm, classes=classes,\n",
    "                      title='Confusion Matrix, without normalization')\n",
    "# plot_confusion_matrix(cm, classes=classes, normalize=True,\n",
    "#                       title='Normalized Confusion Matrix')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe676a86",
   "metadata": {},
   "source": [
    "### Fully connected neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c4ab78",
   "metadata": {},
   "source": [
    "### Convolutional neural network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
